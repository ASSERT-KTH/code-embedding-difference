{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47dbdb76-add0-459d-99b6-ed2f7650901b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-31 13:27:27.961694: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-08-31 13:27:28.048147: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-08-31 13:27:28.048186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-08-31 13:27:28.053458: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import os\n",
    "\n",
    "model_id = \"answerdotai/ModernBERT-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48193c5a-6c0d-4e5d-8e76-4cbe65528283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenized_chunk(save_dir, chunk_num):\n",
    "    chunk_file = os.path.join(save_dir, f'chunk_{chunk_num:04d}.pkl')\n",
    "    with open(chunk_file, 'rb') as f:\n",
    "        chunk_data = pickle.load(f)\n",
    "\n",
    "    current_chunk_size = chunk_data['chunk_size']\n",
    "    \n",
    "    buggy_tokenized = {\n",
    "       'input_ids': chunk_data['input_ids'][:current_chunk_size],\n",
    "       'attention_mask': chunk_data['attention_mask'][:current_chunk_size]\n",
    "    }\n",
    "    \n",
    "    fixed_tokenized = {\n",
    "       'input_ids': chunk_data['input_ids'][current_chunk_size:],\n",
    "       'attention_mask': chunk_data['attention_mask'][current_chunk_size:]\n",
    "    }\n",
    "\n",
    "    return buggy_tokenized, fixed_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e77973-db00-4f0b-b6e6-1fd6b8d6fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_pooled_embeddings(input_ids, attention_mask):\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand_as(hidden).float()\n",
    "        summed = (hidden * mask).sum(1)\n",
    "        counts = mask.sum(1).clamp(min=1e-9)\n",
    "        return summed / counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b9140b2d-dd25-485c-af13-1589bb84d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_pair_file(chunk_num, buggy_arrays, fixed_arrays):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    buggy_embeddings = torch.cat(buggy_arrays, dim=0).cpu().numpy()\n",
    "    fixed_embeddings = torch.cat(fixed_arrays, dim=0).cpu().numpy()\n",
    "    output_file = os.path.join(output_dir, f'buggy_fixed_embeddings_chunk_{chunk_num:04d}.pkl')\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                'buggy_embeddings': buggy_embeddings,\n",
    "                'fixed_embeddings': fixed_embeddings\n",
    "            },\n",
    "            f\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89acb64d-a046-467b-95a0-f1bfe45965d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_in_batches(encodings, batch_size=256):\n",
    "    total = encodings['input_ids'].shape[0]\n",
    "    encodings = {k: v.to(model.device, non_blocking=True) for k, v in encodings.items()}\n",
    "    pooled_outputs = []\n",
    "\n",
    "    for  start_idx in range(0, total, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total)\n",
    "\n",
    "        input_ids_batch = encodings[\"input_ids\"][start_idx:end_idx]\n",
    "        attention_mask_batch = encodings[\"attention_mask\"][start_idx:end_idx]\n",
    "        pooled = get_mean_pooled_embeddings(input_ids_batch, attention_mask_batch)\n",
    "        pooled_outputs.append(pooled)\n",
    "\n",
    "    all_embeddings = torch.cat(pooled_outputs)\n",
    "\n",
    "    return all_embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a0ff9-1007-4f2b-9709-d16a3f2f93dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing chunk 19, batch 0\n",
      "Processing chunk 19, batch 1\n",
      "Processing chunk 19, batch 2\n",
      "Processing chunk 19, batch 3\n",
      "Processing chunk 19, batch 4\n",
      "Processing chunk 19, batch 5\n",
      "Processing chunk 19, batch 6\n"
     ]
    }
   ],
   "source": [
    "tokenized_dir = \"/mimer/NOBACKUP/groups/naiss2025-5-243/tokenized_chunks2\" # Location of pretokenized data\n",
    "output_dir = \"/mimer/NOBACKUP/groups/naiss2025-5-243/buggy_fixed_embeddings\" # Location of finished embeddings\n",
    "\n",
    "step_size = 1000\n",
    "\n",
    "for chunk_num in range(0, 23): # Alter if needed\n",
    "    buggy_data, fixed_data = load_tokenized_chunk(tokenized_dir, chunk_num)\n",
    "    total_size = buggy_data['input_ids'].shape[0]\n",
    "    \n",
    "    buggy_final = []\n",
    "    fixed_final = []\n",
    "\n",
    "    for batch_idx in range(20):\n",
    "        print(f\"Processing chunk {chunk_num}, batch {batch_idx}\")\n",
    "        start = batch_idx * step_size\n",
    "        end = min(start + step_size, total_size)\n",
    "        if start >= end:\n",
    "            break\n",
    "\n",
    "        buggy_batch = {\n",
    "            'input_ids': buggy_data['input_ids'][start:end],\n",
    "            'attention_mask': buggy_data['attention_mask'][start:end]\n",
    "        }\n",
    "        fixed_batch = {\n",
    "            'input_ids': fixed_data['input_ids'][start:end],\n",
    "            'attention_mask': fixed_data['attention_mask'][start:end]\n",
    "        }\n",
    "\n",
    "        buggy_emb = embed_in_batches(buggy_batch)\n",
    "        fixed_emb = embed_in_batches(fixed_batch)\n",
    "\n",
    "        buggy_final.append(buggy_emb)\n",
    "        fixed_final.append(fixed_emb)\n",
    "    print(\"Chunk completed\")\n",
    "    write_pair_file(chunk_num, buggy_final, fixed_final)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f8e847d4-0363-4596-bc16-2093ba78a286",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df4cb08-2440-4a54-a207-cfce34b061d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Testenv3",
   "language": "python",
   "name": "testenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
