{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbc01841-6883-45d3-bc64-30d46e512be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from datasets import Dataset\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7ffa98-3a2a-48e8-8947-ed12b223a14a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"answerdotai/ModernBERT-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModel.from_pretrained(model_id)\n",
    "model.eval()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "torch.set_float32_matmul_precision('high')\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"NicholasOgenstad/my-runbugrun-dataset\",\n",
    "    data_files=\"runbugrun_all_pairs_with_language.json\",\n",
    "    split=\"train\"\n",
    ")\n",
    "dataset = dataset.filter(lambda example: example[\"language\"] != \"tests\")\n",
    "\n",
    "buggy = dataset['buggy_code']\n",
    "fixed = dataset['fixed_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48193c5a-6c0d-4e5d-8e76-4cbe65528283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenized_chunk(save_dir, chunk_num):\n",
    "    chunk_file = os.path.join(save_dir, f'chunk_{chunk_num:04d}.pkl')\n",
    "    with open(chunk_file, 'rb') as f:\n",
    "        chunk_data = pickle.load(f)\n",
    "\n",
    "    current_chunk_size = chunk_data['chunk_size']\n",
    "    \n",
    "    buggy_tokenized = {\n",
    "       'input_ids': chunk_data['input_ids'][:current_chunk_size],\n",
    "       'attention_mask': chunk_data['attention_mask'][:current_chunk_size]\n",
    "    }\n",
    "    \n",
    "    fixed_tokenized = {\n",
    "       'input_ids': chunk_data['input_ids'][current_chunk_size:],\n",
    "       'attention_mask': chunk_data['attention_mask'][current_chunk_size:]\n",
    "    }\n",
    "\n",
    "    return buggy_tokenized, fixed_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e77973-db00-4f0b-b6e6-1fd6b8d6fe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_pooled_embeddings(input_ids, attention_mask):\n",
    "    with torch.no_grad(), torch.autocast(\"cuda\"):\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden = outputs.last_hidden_state\n",
    "        mask = attention_mask.unsqueeze(-1).expand_as(hidden).float()\n",
    "        summed = (hidden * mask).sum(1)\n",
    "        counts = mask.sum(1).clamp(min=1e-9)\n",
    "        return summed / counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9140b2d-dd25-485c-af13-1589bb84d116",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_diff_file(chunk_num, diff_array):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    diff_embeddings = torch.cat(diff_array, dim=0)\n",
    "    diff_embeddings_np = diff_embeddings.cpu().numpy()\n",
    "    \n",
    "    output_file = os.path.join(output_dir, f'diff_embeddings_chunk_{chunk_num:04d}.pkl')\n",
    "    with open(output_file, 'wb') as f:\n",
    "        pickle.dump(diff_embeddings_np, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89acb64d-a046-467b-95a0-f1bfe45965d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_in_batches(encodings, batch_size=192):\n",
    "    total = encodings['input_ids'].shape[0]\n",
    "    if total == 0:  \n",
    "        return torch.empty(0, model.config.hidden_size).cpu()\n",
    "        \n",
    "    encodings = {k: v.to(model.device, non_blocking=True) for k, v in encodings.items()}\n",
    "    pooled_outputs = []\n",
    "    \n",
    "    for  start_idx in range(0, total, batch_size):\n",
    "        end_idx = min(start_idx + batch_size, total)\n",
    "        input_ids_batch = encodings[\"input_ids\"][start_idx:end_idx]\n",
    "        attention_mask_batch = encodings[\"attention_mask\"][start_idx:end_idx]\n",
    "        pooled = get_mean_pooled_embeddings(input_ids_batch, attention_mask_batch)\n",
    "        pooled_outputs.append(pooled)\n",
    "        \n",
    "    all_embeddings = torch.cat(pooled_outputs)\n",
    "    return all_embeddings.cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a69a0ff9-1007-4f2b-9709-d16a3f2f93dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dir = \"/mimer/NOBACKUP/groups/naiss2025-5-243/tokenized_chunks2\" # Pretokenized data location\n",
    "output_dir = \"/mimer/NOBACKUP/groups/naiss2025-5-243/diff_embeddings2\" # Location of embeddings after encoding\n",
    "\n",
    "step_size = 1000\n",
    "\n",
    "for chunk_num in range(22, 23):\n",
    "    \n",
    "    buggy_data, fixed_data = load_tokenized_chunk(tokenized_dir, chunk_num)\n",
    "    total_size = buggy_data['input_ids'].shape[0]\n",
    "    \n",
    "    diff_final = []\n",
    "    batch_idx = 0\n",
    "    while True:\n",
    "        print(f\"Processing chunk {chunk_num}, batch {batch_idx}\")\n",
    "        start = batch_idx * step_size\n",
    "        end = min(start + step_size, total_size)\n",
    "        \n",
    "        if start >= total_size:\n",
    "            break\n",
    "        \n",
    "        buggy_batch = {\n",
    "            'input_ids': buggy_data['input_ids'][start:end],\n",
    "            'attention_mask': buggy_data['attention_mask'][start:end]\n",
    "        }\n",
    "        fixed_batch = {\n",
    "            'input_ids': fixed_data['input_ids'][start:end],\n",
    "            'attention_mask': fixed_data['attention_mask'][start:end]\n",
    "        }\n",
    "    \n",
    "        buggy = embed_in_batches(buggy_batch)\n",
    "        fixed = embed_in_batches(fixed_batch)\n",
    "    \n",
    "        diff_batch = fixed - buggy\n",
    "        diff_final.append(diff_batch)\n",
    "        batch_idx += 1\n",
    "    write_diff_file(chunk_num, diff_final)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3bfd25-d3af-464f-9769-27bfbb66fa86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Testenv3",
   "language": "python",
   "name": "testenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
